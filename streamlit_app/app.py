import streamlit as st
import joblib
import numpy as np
import re
import torch
from transformers import AutoTokenizer, AutoModel
import os # å¯¼å…¥ os æ¨¡å—

# ==== æ¨¡å‹é…ç½®ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰ ====
# ä½¿ç”¨ os.path.join æ¥æ„å»ºè·¯å¾„ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°å¤„ç†ä¸åŒæ“ä½œç³»ç»Ÿçš„è·¯å¾„åˆ†éš”ç¬¦
# os.path.dirname(__file__) è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•
# "models" æ˜¯æ¨¡å‹æ–‡ä»¶æ‰€åœ¨çš„å­æ–‡ä»¶å¤¹å
# è¿™ä¸ªæ–¹æ³•ä¼šæ„å»ºä¸€ä¸ªç»å¯¹è·¯å¾„ï¼Œæ›´å¥å£®
BASE_DIR = os.path.dirname(__file__)

MODEL_OPTIONS = {
    "TF-IDF baseline (no_urls)": {
        "model": os.path.join(BASE_DIR, "models", "lr_model_no_urls.joblib"),
        "vectorizer": os.path.join(BASE_DIR, "models", "tfidf_no_urls.joblib"),
        "scaler": None
    },
    "TF-IDF + Extra Features": {
        "model": os.path.join(BASE_DIR, "models", "lr_model_extra_no_urls.joblib"),
        "vectorizer": os.path.join(BASE_DIR, "models", "tfidf_extra_no_urls.joblib"),
        "scaler": os.path.join(BASE_DIR, "models", "scaler_extra_no_urls.joblib")
    },
    "TF-IDF + BERT + Engineered": {
        "model": os.path.join(BASE_DIR, "models", "lr_tfidf_bert_engineered.joblib"),
        "vectorizer": os.path.join(BASE_DIR, "models", "tfidf_vectorizer_bert_engineered.joblib"),
        "scaler": os.path.join(BASE_DIR, "models", "feature_scaler_bert_engineered.joblib")
    }
}

st.title("Parteivorhersage fÃ¼r Bundestags-Tweets ğŸ‡©ğŸ‡ª")

choice = st.selectbox("ğŸ“¦ WÃ¤hle ein Modell:", list(MODEL_OPTIONS.keys()))
info = MODEL_OPTIONS[choice]

# **é‡è¦ï¼šåœ¨åŠ è½½æ–‡ä»¶å‰æ·»åŠ æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥**
# è¿™æœ‰åŠ©äºåœ¨å¼€å‘æˆ–éƒ¨ç½²æ—¶å¿«é€Ÿå‘ç°æ–‡ä»¶è·¯å¾„é—®é¢˜
try:
    model = joblib.load(info["model"])
    vectorizer = joblib.load(info["vectorizer"])
    scaler = joblib.load(info["scaler"]) if info["scaler"] else None
except FileNotFoundError as e:
    st.error(f"âŒ é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ã€‚è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ¨¡å‹æ–‡ä»¶æ˜¯å¦å·²ä¸Šä¼ ã€‚")
    st.error(f"è¯¦ç»†é”™è¯¯: {e}")
    st.stop() # åœæ­¢åº”ç”¨æ‰§è¡Œï¼Œé¿å…åç»­é”™è¯¯

use_bert = "BERT" in choice

# ==== BERT ====
if use_bert:
    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ try-exceptå—ï¼Œä»¥é˜²æ¨¡å‹ä¸‹è½½æˆ–åŠ è½½å¤±è´¥
    try:
        tokenizer = AutoTokenizer.from_pretrained("bert-base-german-cased")
        bert_model = AutoModel.from_pretrained("bert-base-german-cased")
        bert_model.eval()
    except Exception as e:
        st.error(f"âŒ BERT æ¨¡å‹åŠ è½½å¤±è´¥ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹åç§°ã€‚")
        st.error(f"è¯¦ç»†é”™è¯¯: {e}")
        st.stop()


# ==== Feature Engineering ====
POLITICAL_TERMS = [
    "klimaschutz", "freiheit", "bÃ¼rgergeld", "migration", "rente", "gerechtigkeit",
    "steuern", "digitalisierung", "gesundheit", "bildung", "europa", "verteidigung",
    "arbeitsmarkt", "soziales", "integration", "umweltschutz", "innenpolitik"
]

def count_emojis(text):
    try:
        import emoji
        return sum(1 for char in str(text) if char in emoji.EMOJI_DATA)
    except ImportError:
        # å¦‚æœ emoji åº“æ²¡æœ‰å®‰è£…ï¼Œåˆ™ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ
        return str(text).count(":")

def extract_features(text):
    feats = [
        len(str(text)),
        len(str(text).split()),
        avg_word_length(text),
        uppercase_ratio(text),
        str(text).count("!"),
        str(text).count("?"),
        multi_punct_count(text),
        count_political_terms(text),
        count_emojis(text),
        count_hashtags(text),
        count_mentions(text),
        count_urls(text),
        count_dots(text),
        is_retweet(text),
    ]
    return np.array(feats).reshape(1, -1)

def avg_word_length(text):
    words = re.findall(r"\w+", str(text))
    return sum(len(w) for w in words) / len(words) if words else 0

def uppercase_ratio(text):
    text = str(text)
    return sum(1 for c in text if c.isupper()) / len(text) if text else 0

def multi_punct_count(text): return len(re.findall(r"[!?]{2,}", str(text)))
def count_political_terms(text): return sum(1 for w in POLITICAL_TERMS if w in str(text).lower())
def count_hashtags(text): return len(re.findall(r"#\w+", str(text)))
def count_mentions(text): return len(re.findall(r"@\w+", str(text)))
def count_urls(text): return len(re.findall(r"http\S+|www\S+|https\S+", str(text)))
def count_dots(text): return len(re.findall(r"\.\.+", str(text)))
def is_retweet(text): return int(str(text).strip().lower().startswith("rt @"))

def embed_single_text(text):
    with torch.no_grad():
        encoded = tokenizer(text, truncation=True, padding="max_length", max_length=64, return_tensors="pt")
        output = bert_model(**encoded)
        return output.last_hidden_state[:, 0, :].squeeze().cpu().numpy().reshape(1, -1)

# ==== UI Eingabe ====
st.markdown("âœï¸ **Gib einen Bundestags-Tweet ein:**")
tweet = st.text_area("", placeholder="Wir fordern mehr Klimaschutz und soziale Gerechtigkeit fÃ¼r alle...")

if tweet and st.button("ğŸ”® Vorhersagen"):
    # ç¡®ä¿ scaler å’Œ bert_model åœ¨ä½¿ç”¨å‰å·²è¢«æ­£ç¡®åˆå§‹åŒ–
    X_tfidf = vectorizer.transform([tweet])
    
    X_eng_scaled = None
    if scaler: # åªæœ‰å½“ scaler å­˜åœ¨æ—¶æ‰è¿›è¡Œç‰¹å¾æå–å’Œç¼©æ”¾
        X_eng = extract_features(tweet)
        X_eng_scaled = scaler.transform(X_eng)
    
    X_bert = None
    if use_bert and 'bert_model' in locals(): # ç¡®ä¿ bert_model å˜é‡å·²å®šä¹‰
        X_bert = embed_single_text(tweet)

    # === Merkmals-Kombination ===
    if use_bert and X_bert is not None:
        if X_eng_scaled is not None:
            X_all = np.hstack([X_tfidf.toarray(), X_bert, X_eng_scaled])
        else:
            X_all = np.hstack([X_tfidf.toarray(), X_bert]) # å¦‚æœæ²¡æœ‰ scalerï¼Œåªæœ‰ tfidf å’Œ bert
    elif X_eng_scaled is not None:
        X_all = np.hstack([X_tfidf.toarray(), X_eng_scaled])
    else:
        X_all = X_tfidf.toarray() # ç¡®ä¿è¿™é‡Œä¹Ÿæ˜¯ numpy array

    try:
        pred = model.predict(X_all)[0]
        st.success(f"ğŸ—³ï¸ **Vorhergesagte Partei:** {pred}")

        if hasattr(model, "predict_proba"):
            probs = model.predict_proba(X_all)[0]
            st.subheader("ğŸ“Š Wahrscheinlichkeit je Partei")
            st.bar_chart({p: float(prob) for p, prob in zip(model.classes_, probs)})
    except Exception as e:
        st.error(f"âŒ é¢„æµ‹æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        st.warning("è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å‹å’Œç¼©æ”¾å™¨æ–‡ä»¶éƒ½å·²æ­£ç¡®åŠ è½½ä¸”éç©ºã€‚")


st.markdown("---")
st.markdown("ğŸ” Dieses Tool kombiniert klassische Textmerkmale (TF-IDF), BERT-Embeddings und engineered Features zur Klassifikation von Bundestags-Tweets nach Partei.")
