import streamlit as st
import joblib
import numpy as np
import re
import torch
from transformers import AutoTokenizer, AutoModel
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ£€æŸ¥è®¾å¤‡å¹¶è®¾ç½®
device = torch.device('cpu')  # å¼ºåˆ¶ä½¿ç”¨CPUä»¥é¿å…è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
logger.info(f"Using device: {device}")

@st.cache_resource
def load_models():
    """ç¼“å­˜æ¨¡å‹åŠ è½½ä»¥æé«˜æ€§èƒ½"""
    try:
        # Load artifacts
        MODEL_PATH = "models/lr_tfidf_bert_engineered.joblib"
        VECTORIZER_PATH = "models/tfidf_vectorizer_bert_engineered.joblib"
        SCALER_PATH = "models/feature_scaler_bert_engineered.joblib"
        BERT_PATH = "bert-base-german-cased"

        model = joblib.load(MODEL_PATH)
        vectorizer = joblib.load(VECTORIZER_PATH)
        scaler = joblib.load(SCALER_PATH)

        # åŠ è½½BERTæ¨¡å‹å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å’ŒæŒ‡å®šè®¾å¤‡
        tokenizer = AutoTokenizer.from_pretrained(BERT_PATH)
        bert_model = AutoModel.from_pretrained(BERT_PATH)
        bert_model.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        bert_model.eval()
        
        # ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜
        for param in bert_model.parameters():
            param.requires_grad = False
            
        logger.info("Models loaded successfully")
        return model, vectorizer, scaler, tokenizer, bert_model
        
    except Exception as e:
        logger.error(f"Error loading models: {e}")
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None, None

# Feature engineering as in training
POLITICAL_TERMS = [
    "klimaschutz", "freiheit", "bÃ¼rgergeld", "migration", "rente", "gerechtigkeit",
    "steuern", "digitalisierung", "gesundheit", "bildung", "europa", "verteidigung",
    "arbeitsmarkt", "soziales", "integration", "umweltschutz", "innenpolitik"
]

def count_political_terms(text):
    """è®¡ç®—æ”¿æ²»æœ¯è¯­æ•°é‡"""
    if not text:
        return 0
    text = str(text).lower()
    return sum(1 for word in POLITICAL_TERMS if word in text)

def uppercase_ratio(text):
    """è®¡ç®—å¤§å†™å­—æ¯æ¯”ä¾‹"""
    if not text:
        return 0
    text = str(text)
    if len(text) == 0:
        return 0
    return sum(1 for c in text if c.isupper()) / len(text)

def avg_word_length(text):
    """è®¡ç®—å¹³å‡å•è¯é•¿åº¦"""
    if not text:
        return 0
    words = re.findall(r"\w+", str(text))
    if not words:
        return 0
    return sum(len(w) for w in words) / len(words)

def multi_punct_count(text):
    """è®¡ç®—å¤šé‡æ ‡ç‚¹ç¬¦å·æ•°é‡"""
    if not text:
        return 0
    return len(re.findall(r"[!?]{2,}", str(text)))

def count_emojis(text):
    """è®¡ç®—è¡¨æƒ…ç¬¦å·æ•°é‡"""
    if not text:
        return 0
    try:
        import emoji
        return sum(1 for char in str(text) if char in emoji.EMOJI_DATA)
    except ImportError:
        # fallback: just count colons
        return str(text).count(":")

def count_hashtags(text):
    """è®¡ç®—hashtagæ•°é‡"""
    if not text:
        return 0
    return len(re.findall(r"#\w+", str(text)))

def count_mentions(text):
    """è®¡ç®—@æåŠæ•°é‡"""
    if not text:
        return 0
    return len(re.findall(r"@\w+", str(text)))

def count_urls(text):
    """è®¡ç®—URLæ•°é‡"""
    if not text:
        return 0
    return len(re.findall(r"http\S+|www\S+|https\S+", str(text)))

def count_dots(text):
    """è®¡ç®—è¿ç»­ç‚¹æ•°é‡"""
    if not text:
        return 0
    return len(re.findall(r"\.\.+", str(text)))

def is_retweet(text):
    """æ£€æŸ¥æ˜¯å¦ä¸ºè½¬æ¨"""
    if not text:
        return 0
    return int(str(text).strip().lower().startswith("rt @"))

def extract_features(text):
    """æå–å·¥ç¨‹ç‰¹å¾"""
    if not text:
        text = ""
    
    try:
        feats = [
            len(str(text)),                              # tweet_length_chars
            len(str(text).split()),                      # tweet_length_words
            avg_word_length(text),                       # avg_word_length
            uppercase_ratio(text),                       # uppercase_ratio
            str(text).count("!"),                        # exclamations
            str(text).count("?"),                        # questions
            multi_punct_count(text),                     # multi_punct_count
            count_political_terms(text),                 # political_term_count
            count_emojis(text),                          # num_emojis
            count_hashtags(text),                        # num_hashtags
            count_mentions(text),                        # num_mentions
            count_urls(text),                            # num_urls
            count_dots(text),                            # dots
            is_retweet(text),                            # is_retweet
        ]
        return np.array(feats, dtype=np.float32).reshape(1, -1)
    except Exception as e:
        logger.error(f"Error extracting features: {e}")
        # è¿”å›é»˜è®¤ç‰¹å¾å‘é‡
        return np.zeros((1, 14), dtype=np.float32)

def embed_single_text(text, tokenizer, model, max_len=64):
    """ä½¿ç”¨BERTç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
    if not text:
        return np.zeros((1, 768), dtype=np.float32)
    
    try:
        # ç¡®ä¿è¾“å…¥æ–‡æœ¬æ˜¯å­—ç¬¦ä¸²
        text = str(text).strip()
        if not text:
            return np.zeros((1, 768), dtype=np.float32)
        
        with torch.no_grad():
            # ç¼–ç æ–‡æœ¬
            encoded = tokenizer(
                text, 
                truncation=True, 
                padding="max_length", 
                max_length=max_len, 
                return_tensors="pt"
            )
            
            # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
            encoded = {k: v.to(device) for k, v in encoded.items()}
            
            # å‰å‘ä¼ æ’­
            output = model(**encoded)
            
            # è·å–CLS tokençš„åµŒå…¥
            cls_emb = output.last_hidden_state[:, 0, :].squeeze()
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            cls_emb = cls_emb.cpu().numpy()
            
            # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
            if cls_emb.ndim == 1:
                cls_emb = cls_emb.reshape(1, -1)
                
            return cls_emb.astype(np.float32)
            
    except Exception as e:
        logger.error(f"Error in BERT embedding: {e}")
        # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
        return np.zeros((1, 768), dtype=np.float32)

def predict_party(tweet, model, vectorizer, scaler, tokenizer, bert_model):
    """é¢„æµ‹æ¨æ–‡çš„æ”¿å…šå½’å±"""
    try:
        if not tweet or not tweet.strip():
            return None, None, "è¾“å…¥çš„æ¨æ–‡ä¸ºç©º"
        
        tweet = tweet.strip()
        
        # 1. TF-IDFç‰¹å¾
        X_tfidf = vectorizer.transform([tweet])  # (1, 2000)
        
        # 2. BERTåµŒå…¥
        X_bert = embed_single_text(tweet, tokenizer, bert_model)  # (1, 768)
        
        # 3. å·¥ç¨‹ç‰¹å¾
        X_eng = extract_features(tweet)  # (1, 14)
        X_eng_scaled = scaler.transform(X_eng)
        
        # 4. åˆå¹¶æ‰€æœ‰ç‰¹å¾
        X_all = np.hstack([X_tfidf.toarray(), X_bert, X_eng_scaled])
        
        # 5. é¢„æµ‹
        pred = model.predict(X_all)[0]
        
        # 6. è·å–æ¦‚ç‡ï¼ˆå¦‚æœæ”¯æŒï¼‰
        probs = None
        if hasattr(model, "predict_proba"):
            probs = model.predict_proba(X_all)[0]
            parties = model.classes_
        else:
            parties = None
            
        return pred, (parties, probs), None
        
    except Exception as e:
        error_msg = f"é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        return None, None, error_msg

# ä¸»åº”ç”¨ç¨‹åº
def main():
    st.title("ğŸ—³ï¸ Parteivorhersage fÃ¼r Bundestags-Tweets")
    st.markdown("*ML4B-Projekt: Automatische Parteizuordnung basierend auf Tweet-Inhalten*")
    
    # åŠ è½½æ¨¡å‹
    model, vectorizer, scaler, tokenizer, bert_model = load_models()
    
    if model is None:
        st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
        return
    
    # ç”¨æˆ·ç•Œé¢
    st.markdown("### ğŸ“ Tweet eingeben")
    tweet = st.text_area(
        "Gib einen Bundestags-Tweet ein:",
        height=100,
        placeholder="Beispiel: Wir brauchen mehr Klimaschutz und eine faire Energiewende fÃ¼r alle BÃ¼rger..."
    )
    
    # æ·»åŠ ä¸€äº›ç¤ºä¾‹
    st.markdown("#### ğŸ’¡ Beispiele zum Testen:")
    examples = [
        "Wir mÃ¼ssen die Klimakrise ernst nehmen und jetzt handeln! #Klimaschutz",
        "Die Wirtschaftspolitik muss ArbeitsplÃ¤tze schaffen und Innovation fÃ¶rdern.",
        "Mehr Geld fÃ¼r Bildung und faire LÃ¶hne fÃ¼r alle! #Gerechtigkeit"
    ]
    
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("Beispiel 1", key="ex1"):
            tweet = examples[0]
            st.experimental_rerun()
    with col2:
        if st.button("Beispiel 2", key="ex2"):
            tweet = examples[1]
            st.experimental_rerun()
    with col3:
        if st.button("Beispiel 3", key="ex3"):
            tweet = examples[2]
            st.experimental_rerun()
    
    # é¢„æµ‹æŒ‰é’®
    if st.button("ğŸ” Partei vorhersagen", type="primary"):
        if not tweet or not tweet.strip():
            st.warning("âš ï¸ Bitte gib einen Tweet ein!")
            return
        
        # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
        with st.spinner("Analysiere Tweet..."):
            pred, prob_data, error = predict_party(
                tweet, model, vectorizer, scaler, tokenizer, bert_model
            )
        
        if error:
            st.error(f"âŒ {error}")
            return
        
        if pred:
            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            st.success(f"ğŸ¯ **Vorhergesagte Partei:** {pred}")
            
            # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
            if prob_data[0] is not None and prob_data[1] is not None:
                parties, probs = prob_data
                st.markdown("### ğŸ“Š Wahrscheinlichkeitsverteilung")
                
                # åˆ›å»ºæ¦‚ç‡å­—å…¸å¹¶æ’åº
                prob_dict = {p: float(prob) for p, prob in zip(parties, probs)}
                sorted_probs = dict(sorted(prob_dict.items(), key=lambda x: x[1], reverse=True))
                
                # æ˜¾ç¤ºæ¡å½¢å›¾
                st.bar_chart(sorted_probs)
                
                # æ˜¾ç¤ºè¯¦ç»†æ¦‚ç‡
                st.markdown("#### è¯¦ç»†æ¦‚ç‡:")
                for party, prob in sorted_probs.items():
                    percentage = prob * 100
                    st.write(f"**{party}**: {percentage:.1f}%")
    
    # ä¿¡æ¯éƒ¨åˆ†
    st.markdown("---")
    with st.expander("â„¹ï¸ Ãœber dieses Modell"):
        st.markdown("""
        **Modell-Features:**
        - ğŸ”¤ **TF-IDF Vektorisierung**: Textuelle Inhaltsanalyse
        - ğŸ§  **BERT Embeddings**: Kontextuelle WortreprÃ¤sentationen (German BERT)
        - ğŸ”§ **Engineered Features**: Tweet-LÃ¤nge, Hashtags, Mentions, politische Begriffe, etc.
        
        **Hinweise:**
        - Das Modell wurde auf deutschen Bundestags-Tweets trainiert
        - Die Vorhersage basiert auf einer Kombination verschiedener ML-Techniken
        - Ergebnisse sind WahrscheinlichkeitsschÃ¤tzungen, keine Garantien
        """)
    
    st.markdown("---")
    st.markdown("*Entwickelt fÃ¼r das ML4B-Projekt | Verwendete Technologien: Streamlit, scikit-learn, Transformers, PyTorch*")

if __name__ == "__main__":
    main()

